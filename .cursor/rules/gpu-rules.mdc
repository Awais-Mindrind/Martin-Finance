---
alwaysApply: true
---
# ================================
# Project: Finance GPT with AI Workflows
# Cursor Rules (Strict, CPU-Stable / GPU-In-Progress)
# ================================

# --- PROJECT CONTEXT ---
This project implements a production-grade local LLM system for financial document
analysis using GGUF models and AI workflows.

Current state:
- CPU inference is STABLE and VERIFIED
- GPU inference is the NEXT PHASE and is IN PROGRESS

CPU behavior is the reference baseline. GPU work must not regress CPU stability.

# --- ABSOLUTE RULES ---
- Never assume HTTP 200 OK means inference succeeded.
- Empty model output is always a failure.
- Hidden reasoning or control-token output must be disabled.
- Base model weights must never be modified.
- Pods must be fully reconstructable from non-native storage alone.

# --- CPU BASELINE (LOCKED) ---
- CPU inference path is considered correct and stable.
- Do NOT refactor CPU logic unless a regression is proven.
- CPU output correctness is the ground truth for evaluation.
- All GPU outputs must match CPU outputs semantically.

Breaking CPU behavior is a hard failure.

# --- GPU OBJECTIVES (NEXT PHASE) ---
GPU support must:
- Match CPU-visible output exactly (content, not token timing)
- Never reintroduce hidden reasoning or suppressed output
- Use the same prompts, templates, and inference parameters
- Provide measurable speed or scale improvements

GPU is an optimization layer, not a behavior change.

# --- MODEL & INFERENCE CONSTRAINTS ---
- Use GGUF models only.
- Base model remains immutable.
- Training outputs must be LoRA adapters or merged GGUF variants.
- CPU and GPU must use the SAME model artifacts.
- Do not mix training and inference logic.

# --- GPU-SPECIFIC RULES ---
- GPU usage must be explicit (e.g. n_gpu_layers, device flags).
- GPU memory limits must be respected and logged.
- CPU fallback must remain functional at all times.
- GPU failures must degrade gracefully to CPU.

No GPU-only code paths without CPU parity.

# --- REQUIRED INFERENCE VALIDATION ---
Every inference path (CPU or GPU) must:
- Return visible printable text
- Pass deterministic smoke tests (e.g. “Reply exactly: OK”)
- Validate against:
  - choices[0].message.content
  - choices[0].text
- Treat blank output as a hard error

Validation must be identical across CPU and GPU.

# --- API USAGE RULES ---
Allowed endpoints:
- /v1/chat/completions
- /v1/completions

Disallowed assumptions:
- /health endpoints imply readiness
- Server startup implies correctness

# --- TRAINING RULES ---
- Training data source: PDFs only
- PDFs must be stored on non-native storage
- Training must be reproducible from raw PDFs
- Training artifacts must be separable from the base model

GPU is NOT required for training correctness — only for performance.

# --- EVALUATION RULES ---
- Always run baseline (CPU, base model)
- Always run trained comparison (CPU first, then GPU)
- Prompts must be identical across tests
- GPU output must be semantically equivalent to CPU output
- No subjective evaluation

GPU tests are invalid unless CPU tests pass first.

# --- FILE SYSTEM REQUIREMENTS ---
Required directories:
- /workspace/models/
- /workspace/data/raw_pdfs/
- /workspace/data/processed/
- /workspace/scripts/
- /workspace/logs/

GPU-specific logs must be stored alongside CPU logs.

# --- POD RECONSTRUCTION RULES ---
- Pods may be destroyed at any time
- CPU-only pods must always be rebuildable
- GPU pods must reuse the same storage layout
- Startup must be one command
- Logs must persist outside the pod

# --- CURSOR BEHAVIOR RULES ---
- Prefer minimal, deterministic changes
- Do not refactor CPU logic during GPU work
- Do not introduce GPU-only abstractions
- Preserve prompt formats and inference flags
- Always verify via logs and visible output

# --- NON-GOALS ---
Do NOT:
- Change model behavior to suit GPU quirks
- Trade correctness for speed
- Introduce GPU-only features without CPU parity
- Assume GPU correctness without comparison

# --- DEFINITION OF WORKING ---
CPU working:
- Visible text
- Correct content
- Reproducible pods

GPU working:
- Same visible text as CPU
- Same semantics as CPU
- Faster or more scalable
- No regressions

# --- ADAPTER TESTING CONTEXT (CURRENT) ---
Client has 22 trained adapters at `/workspace/output/adapters_gguf/v3/` (created with ChatGPT help).
Client needs to:
- Test base LLM (no adapter)
- Test B2.gguf specifically
- Test all 22 adapters sequentially
- Switch between adapters easily

## Available Tools (Just Added)
- `verify_adapters`: Verifies setup, lists adapters, checks infrastructure
  - Command: `python3 /app/main.py verify_adapters`
  - Checks: base model, B2.gguf, adapters directory, test binary
- `switch_adapter`: Interactive menu + direct commands for adapter testing
  - Command: `python3 /app/main.py switch_adapter`
  - Modes: interactive menu, `--adapter NAME`, `--all`, `--base-only`
- `test_gguf`: Direct testing (existing, still works)
  - Command: `python3 /app/main.py test_gguf --model PATH [--adapter PATH] [--adapters-dir DIR]`

## Key Paths
- Base model: `/workspace/models/mistral-7b-instruct-v0.2.Q4_K_M.gguf`
- B2 adapter: `/workspace/output/adapters_gguf/v3/B2.gguf`
- All adapters: `/workspace/output/adapters_gguf/v3/`
- PEFT adapters (source): `/workspace/output/peft/` (if need to export)

## Current Limitations
- Sequential testing only (restarts inference for each adapter)
- No runtime hot-swap via `/lora-adapters` endpoint (not available in current build)
- Adapter export pipeline (`export_lora.py`) has issues with `llama-export-lora` format flag

## Testing Workflow
1. Verify: `python3 /app/main.py verify_adapters`
2. Test base: `python3 /app/main.py test_gguf --model /workspace/models/mistral-7b-instruct-v0.2.Q4_K_M.gguf`
3. Test B2: `python3 /app/main.py switch_adapter --adapter B2`
4. Test all: `python3 /app/main.py switch_adapter --all`
5. Interactive: `python3 /app/main.py switch_adapter` (menu-driven)

## Notes for Cursor
- Client's adapters are already GGUF format (created externally)
- No test adapters were created by this codebase
- Focus on making testing easy and reliable
- Preserve existing test_gguf.py functionality
- When updating docs, sync README.md, ADAPTER_TESTING_GUIDE.md, and this file

# --- CORE PRINCIPLE ---
CPU defines correctness.
GPU optimizes execution.
If output cannot be seen, compared, and reproduced — it does not exist.
