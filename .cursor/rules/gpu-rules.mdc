---
alwaysApply: true
---
## Cursor Rules: Martin Finance LLM Pods (GPU/CPU)

### Scope and Goal
Keep this doc aligned with the current chat context. It must guide Cursor to:
- support the client’s testing needs,
- avoid breaking existing pipelines,
- update docs/scripts safely,
- and resolve future issues with clarity.

### Client Requirements (from latest messages)
- Test the base LLM (no adapters).
- Test with a specific adapter: `/workspace/output/adapters_gguf/v3/B2.gguf`.
- Be able to switch/test across all adapters in `/workspace/output/adapters_gguf/v3/`.
- Provide a clear guide and working commands so the client can do this independently.

### What We Achieved
- Base model serving works (via `/v1/chat/completions`).
- Added adapter-aware test runner:
  - `app/scripts/test_gguf.py` supports `--adapter`, `--adapters-dir`, GPU detection, and prompt/temperature/max tokens.
  - `app/main.py` forwards adapter/test flags and defaults to `/workspace/models/mistral-7b-instruct-v0.2.Q4_K_M.gguf`.
- Updated docs:
  - `layering-guide.md` now reflects *current limitations* (no hot-swap endpoints).
  - `README.md` includes commands for base, single adapter, and batch adapter testing.
- Confirmed that `/lora-adapters` endpoint is **not** available in the current Python server build. Dynamic hot-swap is not working.
- Confirmed that llama.cpp C++ server runs, but adapter switching is **not** set up end-to-end (adapter GGUF export still required).

### What’s Still Pending / Open
- Adapter GGUF export pipeline:
  - The client’s adapters exist in `/workspace/output/peft/*` (PEFT folders).
  - The GGUF adapter files in `/workspace/output/adapters_gguf/v3/` must exist for testing.
  - We attempted `llama-export-lora --format gguf` and hit `error: invalid argument: --format`. Need correct usage for the current llama.cpp build.
- Reliable adapter switching:
  - Python `llama_cpp.server` lacks adapter endpoints in this build.
  - C++ `llama-server` can be used, but must be paired with valid adapter GGUFs and the correct adapter endpoint path (check server docs/OpenAPI).
- Test runner `test_gguf.py` depends on a working `llama-cli` binary. We saw `SIGILL` due to CPU instruction mismatch; rebuild `llama.cpp` with conservative flags if needed.

### Current Guidance to Client (Stable Path)
- GPU pod: run `python /app/main.py train_all` to produce GGUF.
- CPU pod: set `MODEL_PATH` to GGUF and serve via `/start.sh`, test with `/v1/chat/completions`.
- Adapter hot-swap: not available in current build; static `--lora` only (if compatible adapter GGUFs are produced).

---

## RunPod Workspace Layout (Observed)

### Common paths
- `/workspace/models/`
  - Base GGUFs:
    - `mistral-7b-instruct-v0.2.Q4_K_M.gguf` (default)
    - `mistral-7b-instruct-v0.2.Q4_K_M.f16.gguf`
    - `mistral-7b-instruct-v0.3.Q6_K.gguf`
    - `mistral-7b.Q4_K_M.gguf`
    - `Mistral-7B-Instruct-v0.3.Q6_K.gguf`
    - `TinyLlama-1.1B-Chat-v1.0.Q4_K_M.gguf`
  - HF sources: `hf_mistral/`, `mistral-7b-instruct/`, `mistral-7b-instruct-v0.3/`, `tinyllama-1.1b-chat/`
- `/workspace/output/`
  - `/workspace/output/peft/` (adapter folders, e.g. `ASC_*`, `B2`, `F1`, `M1`, etc.)
  - `/workspace/output/adapters_gguf/v3/` (expected adapter GGUFs; currently empty in logs)
- `/workspace/llama.cpp/` (llama.cpp repo + build output)
- `/workspace/data/`
  - `/workspace/data/raw_pdfs/`
  - `/workspace/data/processed/` (train.jsonl, pdf_pretest.json)
  - `/workspace/data/archive/`

---

## CPU Serving Template (Separate Container)

Use the CPU template image to serve the GGUF model:
- Image: `docker.io/awais2512/martin-finance-grand:v1`
- Startup: `/start.sh` (starts sshd, ttyd, n8n, Qdrant, and `python -m llama_cpp.server` on port 5000)
- Ports: 5000 (LLM), 5678 (n8n), 6333 (Qdrant), 22 (SSH), 7681 (web terminal)
- Env:
  - `MODEL_PATH=/workspace/models/mistral-7b-instruct-v0.2.Q4_K_M.gguf`
  - Optional: `SSH_PUBLIC_KEY` or `SSH_PASSWORD`, `SSH_PORT`, `TTYD_PORT`, `N_THREADS`, `N_CTX`
- Sanity check:
  ```
  curl -s http://127.0.0.1:5000/v1/chat/completions \
    -H "Content-Type: application/json" \
    -d '{"model":"local","messages":[{"role":"user","content":"Give me two facts about Mars"}],"max_tokens":128}'
  ```

Note: Adapter hot-swap is not available in the current Python server build. Use base GGUF only or static `--lora` if adapter GGUFs exist.

---

## Key Commands (Current)

### Base model test (GPU/CPU)
```
python /app/main.py test_gguf \
  --model /workspace/models/mistral-7b-instruct-v0.2.Q4_K_M.gguf
```

### Single adapter test (requires adapter GGUF file)
```
python /app/main.py test_gguf \
  --model /workspace/models/mistral-7b-instruct-v0.2.Q4_K_M.gguf \
  --adapter /workspace/output/adapters_gguf/v3/B2.gguf
```

### Batch adapter test (requires adapter GGUFs)
```
python /app/main.py test_gguf \
  --model /workspace/models/mistral-7b-instruct-v0.2.Q4_K_M.gguf \
  --adapters-dir /workspace/output/adapters_gguf/v3
```

---

## Cursor Rules (Do / Don’t)

Do:
- Preserve current working base-model flow.
- Keep docs consistent with real capabilities (no mention of `/lora-adapters` unless it actually works).
- Ensure any new commands are valid in the pod’s environment.
- Use `python3 /app/main.py` (not `python3/app/main.py`).
- When editing docs, update `README.md` and `layering-guide.md` consistently.

Don’t:
- Claim adapter hot-swap works unless `/docs` or `/openapi.json` shows it.
- Assume adapters in `/workspace/output/peft/` are directly usable as GGUFs.
- Force GPU offload (`-ngl`) on CPU pods.

---

## Next Steps (Recommended)
- Export PEFT adapters to GGUF in `/workspace/output/adapters_gguf/v3/` using the correct `llama-export-lora` usage for the current llama.cpp build.
- Validate adapter testing with `test_gguf.py`.
- If needed, rebuild llama.cpp with conservative CPU flags to avoid SIGILL.
